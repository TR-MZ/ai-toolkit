{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YOUR_USERNAME/ai-toolkit-rgba/blob/main/colab_webui_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ðŸŽ¨ AI Toolkit Web UI on Colab\n",
        "\n",
        "Run the full ai-toolkit training & generation web UI on **Colab's GPU** with public internet access.\n",
        "\n",
        "Features:\n",
        "- âœ… FLUX.2 Klein 4B/9B training\n",
        "- âœ… RGBA transparent image support\n",
        "- âœ… LoRA training & inference\n",
        "- âœ… Public web UI access from anywhere\n",
        "- âœ… Full Colab GPU acceleration\n",
        "\n",
        "**Time to start**: ~5 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## ðŸ“‹ Setup\n",
        "\n",
        "### Step 1ï¸âƒ£: Check GPU (Run this first)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-gpu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import subprocess\n",
        "\n",
        "# Check GPU\n",
        "print(\"ðŸ–¥ï¸  GPU Info:\")\n",
        "print(f\"   Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only'}\")\n",
        "print(f\"   CUDA: {torch.version.cuda}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(\"\\nâœ… GPU available!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  WARNING: No GPU detected! Training will be slow.\")\n",
        "    print(\"   Make sure to enable GPU in Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clone-header"
      },
      "source": [
        "### Step 2ï¸âƒ£: Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone-repo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Clone the repo\n",
        "repo_url = \"https://github.com/YOUR_USERNAME/ai-toolkit-rgba.git\"  # â† UPDATE THIS\n",
        "repo_path = \"/content/ai-toolkit\"\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    print(f\"ðŸ“¥ Cloning from {repo_url}...\")\n",
        "    subprocess.run([\"git\", \"clone\", repo_url, repo_path], check=True)\n",
        "    print(\"âœ… Clone complete!\")\n",
        "else:\n",
        "    print(\"âœ… Repository already cloned\")\n",
        "\n",
        "os.chdir(repo_path)\n",
        "print(f\"ðŸ“‚ Working in: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-header"
      },
      "source": [
        "### Step 3ï¸âƒ£: Install Dependencies\n",
        "\n",
        "â±ï¸ This takes ~3-5 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "install-output"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"ðŸ“¦ Installing Python dependencies...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyngrok\"], check=True)\n",
        "print(\"   âœ… pyngrok\")\n",
        "\n",
        "# Install Node.js if not present\n",
        "result = subprocess.run([\"node\", \"--version\"], capture_output=True)\n",
        "if result.returncode != 0:\n",
        "    print(\"   Installing Node.js...\")\n",
        "    subprocess.run([\"apt-get\", \"update\", \"-qq\"], capture_output=True)\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"-qq\", \"nodejs\", \"npm\"], check=True)\n",
        "\n",
        "print(\"   âœ… Node.js\")\n",
        "\n",
        "print(\"\\nðŸ“¦ Installing npm dependencies for UI...\")\n",
        "subprocess.run([\"npm\", \"install\", \"--prefix\", \"ui\"], capture_output=True, check=False)\n",
        "print(\"   âœ… npm packages\")\n",
        "\n",
        "print(\"\\nâœ… All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "build-header"
      },
      "source": [
        "### Step 4ï¸âƒ£: Build Web UI\n",
        "\n",
        "â±ï¸ This takes ~2-3 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "build-ui"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"ðŸ”¨ Building Next.js UI...\")\n",
        "result = subprocess.run(\n",
        "    [\"npm\", \"run\", \"build\", \"--prefix\", \"ui\"],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"âœ… Build successful!\")\n",
        "else:\n",
        "    print(\"âš ï¸  Build warnings (this is usually fine):\")\n",
        "    if result.stderr:\n",
        "        print(result.stderr[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-header"
      },
      "source": [
        "### Step 5ï¸âƒ£: Start Web UI with Public Tunnel\n",
        "\n",
        "ðŸš€ This starts the server and creates a public URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-server",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "server-output"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "import os\n",
        "import signal\n",
        "\n",
        "UI_PORT = 8675\n",
        "ui_process = None\n",
        "\n",
        "def start_ui_server():\n",
        "    \"\"\"Start the Next.js UI server\"\"\"\n",
        "    global ui_process\n",
        "    os.chdir(\"/content/ai-toolkit\")\n",
        "    ui_process = subprocess.Popen(\n",
        "        [\"npm\", \"run\", \"start\", \"--prefix\", \"ui\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    ui_process.wait()\n",
        "\n",
        "print(\"ðŸš€ Starting web UI server...\")\n",
        "ui_thread = Thread(target=start_ui_server, daemon=False)\n",
        "ui_thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "print(\"â³ Waiting for server to start (30 seconds)...\")\n",
        "for i in range(30):\n",
        "    time.sleep(1)\n",
        "    if i % 5 == 0:\n",
        "        print(f\"   {i}s...\")\n",
        "\n",
        "# Create ngrok tunnel\n",
        "print(\"\\nðŸŒ Creating public tunnel with ngrok...\")\n",
        "try:\n",
        "    public_url = ngrok.connect(UI_PORT, \"http\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"âœ… WEB UI IS LIVE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nðŸ”— PUBLIC URL:\\n\\n   {public_url}\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"\\nðŸ“ You can now:\")\n",
        "    print(\"   â€¢ Open the URL in your browser\")\n",
        "    print(\"   â€¢ Create training jobs\")\n",
        "   print(\"   â€¢ Generate images with LoRA\")\n",
        "    print(\"   â€¢ Monitor GPU usage\")\n",
        "    print(\"\\nâ±ï¸  Keep this cell running to keep the server alive!\")\n",
        "    print(\"   (You can open new cells to run other commands)\\n\")\n",
        "    \n",
        "    # Keep server alive\n",
        "    ui_thread.join()\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nâ¹ï¸  Server stopped\")\n",
        "    ngrok.kill()\n",
        "    if ui_process:\n",
        "        ui_process.terminate()\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage-header"
      },
      "source": [
        "## ðŸŽ¯ Using the Web UI\n",
        "\n",
        "Once the server is running (Step 5):\n",
        "\n",
        "1. **Copy the PUBLIC URL** from the output above\n",
        "2. **Open it in your browser** (or from your phone!)\n",
        "3. **Create a training job**:\n",
        "   - Select FLUX.2 Klein 4B as the model\n",
        "   - Upload training images (RGBA PNG for transparency)\n",
        "   - Configure LoRA settings\n",
        "   - Start training\n",
        "4. **Monitor in real-time** - watch the GPU usage and sample generations\n",
        "\n",
        "### Features Available\n",
        "\n",
        "âœ… **Training**\n",
        "- FLUX.2 Klein 4B/9B models\n",
        "- LoRA fine-tuning\n",
        "- Custom RGBA VAE support\n",
        "- Real-time sample generation\n",
        "\n",
        "âœ… **Generation**\n",
        "- Apply trained LoRAs\n",
        "- Control images support\n",
        "- Transparent (RGBA) output\n",
        "- Batch generation\n",
        "\n",
        "âœ… **Monitoring**\n",
        "- GPU/VRAM usage\n",
        "- Training loss curves\n",
        "- Sample preview\n",
        "- Job history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper-header"
      },
      "source": [
        "## ðŸ› ï¸ Helper Commands\n",
        "\n",
        "Run these in separate cells while the server is running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-gpu-usage"
      },
      "outputs": [],
      "source": [
        "# Check current GPU usage\n",
        "import subprocess\n",
        "\n",
        "print(\"ðŸ“Š Current GPU Status:\")\n",
        "result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
        "print(result.stdout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list-jobs"
      },
      "outputs": [],
      "source": [
        "# Check training job status\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(\"/content/ai-toolkit/output\")\n",
        "if output_dir.exists():\n",
        "    print(\"ðŸ“ Training outputs:\")\n",
        "    for folder in sorted(output_dir.iterdir()):\n",
        "        if folder.is_dir():\n",
        "            print(f\"\\n   ðŸ“‚ {folder.name}\")\n",
        "            # Show latest sample\n",
        "            samples = list(folder.glob(\"*.png\"))\n",
        "            if samples:\n",
        "                latest = max(samples, key=lambda p: p.stat().st_mtime)\n",
        "                print(f\"      Latest sample: {latest.name}\")\n",
        "else:\n",
        "    print(\"No training outputs yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tips"
      },
      "source": [
        "## ðŸ’¡ Tips\n",
        "\n",
        "- **Keep the notebook running**: The server cell needs to stay active. You can open new cells to run commands.\n",
        "- **URL is temporary**: The ngrok URL changes each session. It's unique to this Colab session.\n",
        "- **VRAM**: Colab usually has ~16GB VRAM. Perfect for FLUX.2 Klein 4B!\n",
        "- **Download results**: Check `/content/ai-toolkit/output/` for all generated files\n",
        "- **GPU limits**: Colab free tier has usage limits. Paid Colab Pro has no limits.\n",
        "\n",
        "## â“ Troubleshooting\n",
        "\n",
        "**\"Can't connect to URL\"**\n",
        "- Make sure the server cell is still running\n",
        "- Wait 30+ seconds for the server to fully start\n",
        "- Try opening in an incognito tab\n",
        "\n",
        "**\"Module not found\" errors**\n",
        "- Re-run Step 3 (Install Dependencies)\n",
        "\n",
        "**GPU out of memory**\n",
        "- Restart runtime (Runtime > Restart runtime)\n",
        "- Make sure you're using GPU T4 or better (not CPU)\n",
        "\n",
        "**Server won't start**\n",
        "- Check Step 4 build output for errors\n",
        "- Restart the notebook (Runtime > Restart runtime)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
